---
title             : "FAIR Theory: Applying Open Science Principles to the Construction and Iterative Improvement of Scientific Theories"
shorttitle        : "FAIR THEORY"

author:
  - name: "Caspar J. Van Lissa"
    affiliation: "1"
    corresponding: yes
    address: "Professor Cobbenhagenlaan 125, 5037 DB Tilburg, The Netherlands"
    email: "c.j.vanlissa@tilburguniversity.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Formal Analysis
      - Funding acquisition
      - Methodology
      - Project administration
      - Software
      - Supervision
      - Writing – original draft
      - Writing – review & editing
  - name: "Aaron Peikert"
    affiliation: "2,3"
    role:
      - Formal Analysis
      - Writing – original draft
      - Writing – review & editing
  - name: "Andreas M. Brandmaier"
    affiliation: "2,3,4"
    role:
      - Formal Analysis
      - Writing – original draft
      - Writing – review & editing
  - name: "Felix D. Schönbrodt"
    affiliation: "5"
    role:
      - Conceptualization
      - Writing – review & editing
affiliation:
  - id            : "1"
    institution   : "Tilburg University, dept. Methodology & Statistics"
  - id            : "2"
    institution   : "Center for Lifespan Psychology, Max Planck Institute for Human Development, Berlin, Germany"
  - id            : "3"
    institution   : "Max Planck UCL Centre for Computational Psychiatry and Ageing Research, Berlin, Germany"
  - id            : "4"
    institution   : "Department of Psychology, MSB Medical School Berlin, Berlin, Germany"
  - id            : "5"
    institution   : "Other affiliations"
authornote: |
  This is a preprint paper, generated from Git Commit # `r substr(gert::git_commit_id(),1,8)`.

abstract: |
  Test test.
keywords          : "meta theory, theory formation, cumulative science, formal models"
wordcount         : "`r tryCatch(wordcountaddin::word_count(here::here('manuscript.Rmd')))`"
bibliography      : "theory-specification.bib"
floatsintext      : yes
linenumbers       : yes
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```


The FAIR Guiding Principles (hereafter: FAIR principles) were established to improve the reusability of research data by making them more findable, accessible, interoperable and reusable [REF] for both humans and computers.
Since their inception in 2014, scholars have demonstrated their relevance for making other information artefacts more open, such as research software [@lamprecht2020towards; @barker2022introducing], computational workflows [@wilkinson2024applying] or simulation studies [@amaro2024need].
This paper argues that the FAIR principles can advance effective and transparent scholarly communication about theory.
To this end, we introduce "FAIR theory":
a digital representation of a scientific theory, compliant with the FAIR principles.
By improving the efficiency of scholarly communication, FAIR Theory has the potential to foster and accelerate cumulative knowledge acquisition and ultimately advance social scientific research.

<!-- QUOTES 

Kurt Lewin: Nothing is as practical as a good theory

"Call Leon": When finding unexpected results in cognitive dissonance research, students were told to call Leon Festinger. He could explain why the study did not work, e.g. because you used the wrong font for the questionnaire.

END QUOTES -->

## The Need for FAIR Theory

The so-called "replication crisis" has prompted extensive reforms in social science [@lavelleWhenCrisisBecomes2021; @scheelWhyMostPsychological2022].
Concern that undisclosed flexibility in analyses was a major factor for the abundance of non-replicable findings led to widespread adoption of open science practices like preregistration and replication [@nosekPromotingOpenResearch2015a].
These various practices ensure transparent and repeated testing of hypotheses.
However, recent reviews show that most preregistered hypothesis tests are not supported by empirical evidence [@scheelExcessPositiveResults2021].
Thus, increased rigor in testing has revealed that the root cause of the replication crisis is more fundamental:
Psychological theories rarely produce hypotheses that are corroborated by evidence.
Furthermore, theories are often so vague that they can accommodate findings that are mutually inconsistent,
as the theory's central claims evade falsification.<!--CJ: Resolved AB's comment? AB: Your original wording is contradicting itself; if findings contradict a theory <=> the theory does not explain the findings; I am not sure what the best wording is; should we differentiate between a core of a theory and auxiliary parts?-->
<!-- CJ: Maybe we should give an example here. Does anyone know one? -->

Scholars have been raising concerns about the state of theory in social science for nearly 50 years [@robinaughInvisibleHandsFine2021; @meehlTheoreticalRisksTabular1978].
Two main concerns are that, first, social scientific theories lack precision compared to theories in the physical sciences [@szollosiArrestedTheoryDevelopment2021].
 and clarity
In other words, social scientific theories lack *formalization*,
which means that they do not make very accurate predictions,
and are thus hard to falsify and difficult to understand on their own,
without either substantial interpretation or additional background knowledge.
A second concern is the lack of transparent and participative scholarly communication about psychological theory and their development over time.

Given these concerns, it is unfortunate that scientific reform initiated by the open science movement has focused primarily on improving deductive methods.
The equally critical inductive processes of theory construction and theory improvement have been largely overlooked.
The present paper restores balance by applying, for the first time,
open science principles to psychological theory.
We apply the FAIR principles to scientific theories,
introducing the concept of *FAIR Theory* to 
facilitate transparent scholarly communication and accelerate cumulative knowledge acquisition.

## Theory and Scientific Progress

According to the *empirical cycle* [@degrootMethodologieGrondslagenVan1961],
a philosophical model of cumulative knowledge acquisition,
research ideally follows a cyclical process with two phases (Figure \@ref(fig:figec)).
In the deductive phase, hypotheses derived from theory are tested on data. In the inductive phase, patterns observed in data are generalized to theoretical principles.
In this model, theories are the vehicle of scientists' understanding of phenomena.
Ideally, they are iteratively updated based on deductive testing and inductive theory construction.

```{r}
library(tidySEM)
library(ggplot2)
y_spacing <- .5
lo <- get_layout("", "A", "",
                 "D", "", "B",
                 "", "C", "", rows = 3)

edg <- data.frame(from = c("A", "B", "C", "D"),
                  to = c("B", "C", "D", "A")
)

p <- prepare_graph(layout = lo, edges = edg)
p$edges[] <- lapply(p$edges, unlist)
p$edges$connect_from <- list("right", "bottom", "left", "top")
p$edges$connect_to <- list("top", "right", "bottom", "left")
p$edges$curvature <- c(rep(60, 4))
p$edges$label <- c("deduction", "testing", "induction", "generalization")
p$edges$label_colour <- "gray50"
p$edges$colour <- "gray50"

p$nodes$label <- c("Theory", "Hypothesis", "Data", "Observed\npatterns")

p_old <- p
g <- plot(p)

topofplot <- max(p$nodes$node_ymax)

headers <- data.frame(
  x = c(3, 5),
  y = topofplot + 2*y_spacing,
  lab = c("Inductive phase", "Deductive phase")
)

letters <- p$nodes[, c("name", "x", "y")]
letters$y <- letters$y+.26
letters$x <- letters$x+.5
g <- g + geom_hline(yintercept = topofplot+y_spacing) +
  geom_vline(xintercept = median(p$nodes$x), linetype = 5) +
  geom_text(data = headers, aes(x = x, y = y, label = lab), vjust = 1) +
  #geom_label(data = letters, aes(x = x, y = y, label = name), fill = "lightblue") +
  scale_x_continuous(expand = c(0,0))+
  scale_y_continuous(expand = c(0,0))
g$layers <- g$layers[c(9, 1:8, 10:length(g$layers))]
ggsave("empirical_cycle.pdf", g, device = "pdf", width = 200, height = 200, units = "mm")
```

```{r figec, fig.cap="A take on the empirical cycle by De Groot"}
knitr::include_graphics("empirical_cycle.pdf")
```

In a progressive research program [@lakatosHistoryScienceIts1971],
this cycle is regularly completed to iteratively advance our understanding of the studied phenomena
<!-- by successively increasing the empirical content of a theory. -->
<!-- In psychology, this is typically achieved by either generalizing to a larger set of people, contexts, situations, or environments. <!--AB would you agree?-->
<!-- CJ: I am not sure if I agree; I think increasing the empirical content rather implies more and more precision, which loses generality but gains specificity. Not sure how to resolve.
This needs more work! -->
.
There are, however, indications that contemporary psychology falls short of this ideal.
Firstly, because deductive research is over-represented in the literature: 
According to @kuhbergerPublicationBiasPsychology2014, 89.6% of papers published in psychology report confirmatory hypothesis tests.
Closer examination of reported deductive research reveals, however, that the link between theory and hypothesis is often tenuous [@scheelWhyHypothesisTesters2021; @oberauerAddressingTheoryCrisis2019].
Only 15% of deductive studies referenced any theory, and theory was often not cited in relation to the hypothesis [@mcphetresDecadeTheoryReflected2021].
The remaining 85% of deductive studies lacked an explicit derivation chain from theory to hypothesis.
In the best case, such ungrounded hypotheses are rooted in researchers' implicit theories, in which case it is particularly important to make these explicit [@friedTheoriesModelsWhat2020, @norouziCapturingCausalClaims2024].
Or, perhaps the hypotheses are not of substantive interest, such as null hypotheses that exist purely for the purpose of being rejected [@vanlissaTeacherCornerEvaluating2020], and researchers are simply testing them as part of a cultural ritual [@gigerenzerNullRitualWhat2004].
Testing ad-hoc hypotheses not grounded in theory does not advance our principled understanding of psychological phenomena.
Or to adapt an analogy of @van2021theory, just collecting significance statements about ad-hoc hypotheses is much like trying to write novels by collecting sentences from randomly generated letter strings.

Theory thus has an uncomfortable and paradoxical role in contemporary psychology:
The majority of papers ostensibly test hypotheses,
but these are rarely derived from theory,
and test results do not routinely contribute to the improvement of existing theories.
The paradoxical role of theory in psychology is perhaps best described by Meehl's observation that theories in psychology "lack the cumulative character of scientific knowledge. They tend neither to be refuted nor corroborated, but instead merely fade away as people lose interest" [@meehlTheoreticalRisksTabular1978].


<!-- First, we reflect on the differences between theory and data. <!-- CJ: Should we? --> -->
<!-- Second, we adapt the FAIR principles for theory, respecting these differences. -->
<!-- Third, we discuss the current state and future directions of psychological theory with respect to each criterion. -->
<!-- Fourth, we present worked examples of theory made FAIR. -->
<!-- We conclude with a summary and future directions for FAIR theory. -->

## Making Theory FAIR

The present paper addresses the lack of open science methods for theory development and suggests an improvement of the state of affairs by applying the FAIR principles to scientific theories.
Merely publishing theory -- as in a classic research article -- does not make it open;
to be open, theory should adhere to established open science standards.
<!--AB  I deleted this sentence because it was already in the introduction -->
We apply the FAIR principles to digital representations of theory,
introducing a FAIR metadata format to make theories *Findable* via a DOI,
*Accessible* in a machine- and human-readable filetype,
*Interoperable* within the data analysis environment,
and *Reusable* in the practical and legal sense, so that they may be improved over time -- at best, in a participative process.
Digital representations of theory intentionally is a broad term, particularly including textual representations of a given theory, as well as formal representations, such as mathematical notation, algorithmic pseudo code, or a set of logical clauses. <!-- AB I think some initial word on what is covered is needed here; otherwise readers may wonder what we mean to cover--> 
Following the original proposal of Lamprecht and colleagues,
we adapt the FAIR principles for theory, see Table \@ref(tab:tabfair).
We reflect on the necessary changes (which are minor),
as well as on the current state and future of FAIR theory in the social sciences.
The resulting principles provide guidance for instantiating theory as a FAIR information artifact,
and we provide worked examples to encourage their adoption.

```{r tabfair}
tab_fair <- read.csv("fair_principles.csv", stringsAsFactors = FALSE)[, c("Criterion", "Original", "Theory", "Our.action")]
names(tab_fair) <- c("Criterion", "Original", "Theory", "Action")
papaja::apa_table(tab_fair, landscape = TRUE, align = "m{.1\\linewidth}m{.35\\linewidth}m{.35\\linewidth}m{.15\\linewidth}")
```


There are different definitions of theory,
and many of those definitions are consistent with the FAIR theory principles.
This paper defines theory as an integrated set of statements that explain phenomena consistently evidenced by patterns in data [bogen/woodward]. <!-- what is this ref exactly?! -->
<!-- Some have defined a model as a "specific instantiation of theory narrower in scope and often more concrete, commonly applied to a particular aspect of a given theory" [REF Fried]. -->
<!-- This invites the question: if a FAIR theory is a specific instantiation of theory, how does FAIR theory differ from a model?
There is no principled difference, but a FAIR theory is the most general instance of a theory, whereas a model is highly specific - e.g., by assuming functional form and error families.
We can make a distinction between statistical models - which make just enough assumptions to allow estimation of the remaining unknown parameters from data - and generative/computational models, which are completely parametrized to the point that a specific interpreter can use them to generate new data.
To some extent, -->
Meehl [-@meehlAppraisingAmendingTheories1990] provides guidance as to what kinds of "statements" such a theory might contain:
statements about the types of entities postulated (i.e., ontology),
statements about causal connections between those entities,
statements about the functional form of those connections,
and statements about their specific numerical values [cf. @frankenhuisStrategicAmbiguitySocial2023, @guestWhatMakesGood2024].
Some have defined a model as a "specific instantiation of theory narrower in scope and often more concrete, commonly applied to a particular aspect of a given theory" [REF Fried].
From this definition, it becomes clear that FAIR models are a special case of FAIR theory.
Yet, theory intentionally has a broader scope and may contain one or more models as specific instances.
For example, a comprehensive theory of disease spread and pandemics may cover various psychological factors such as adherence to pandemic mitigation methods (e.g., ), pandemic-related social disruption (e.g., panic buying), or pandemic-related distress and related problems (e.g., anxiety) [@taylor2022psychology].
The theory may encompass a particular transmission _model_ for disease spread including precise parameters for the process of infection (e.g., social distance, average duration of encounters, ventilation) and incubation times.


### The Role of Theory Formalization

Concerns about the state of theory in the psychological literature revolve around two issues: theory formalization and theory (re-)use.
Greater formalization increases theories' *empirical content* [REF] as it forces researchers to use precise statements, for example by specifying exact functional forms for relations or forces them to specify processes  that would otherwise get away with their vague formulation in verbal theories.
For example, the phonological loop in Baddeley's verbal description of his working memory model allows at least 144 different implementations, for example, depending on how decay rate, recall success, or rehearsal sequence are precisely implemented [@lewandowsky2010computational].
More precise theories makes them easier to falsify,
which necessitates revising them,
thus advancing our principled understanding of the phenomena they describe.
FAIR Theory does not require theories to be formal,
and formal theory can be represented in a way that is not FAIR.
It is therefore important for us to emphasize that FAIR Theory imposes no restrictions on researchers regarding the manner in which theories are derived and written down. 
The guidelines introduced by FAIR Theory primarily pertain to how theories are documented and shared in digital environments, with the aim of enhancing their reusability and extensibility.
For example, it is possible to represent a collection of verbal propositions (perhaps derived through qualitative research) as a FAIR theory.
Conversely, a directed acyclic graph (DAG) is a type of formal theory,
but if it is embedded within a journal article as a bitmap image without any key words to help search engines index that article as a theory paper,
then this formal theory is not FAIR.
FAIR Theory is thus consistent with, but does not require, formal theory (also see *Accessibility*).

## Findability

Making theories Findable would allow researchers to easily identify relevant theories to inform their hypotheses,
grounding their work in established theoretical foundations.
Making theories Findable also increases the impact and reuse potential of theories across disciplines,
either through direct application (where one discipline stumbles upon a problem that is already well-understood in another discipline),
or through analogical modeling.
In analog modeling, the structure of a theory from one discipline is applied to a phenomenon in another field.
For example, predator-prey models have inspired theories of XXX, and the Eysenck model of atomic magnetism has inspired a network theory of depression.
Findability also enables meta-research on theories, 
in the same way libraries and search engines have enabled scholars to study the literature via systematic reviews.
In a similar way, it would become possible to compare all theories of a specific phenomenon,
or to study structural properties of theories.

The four Findability criteria are applicable to theory with only minor adjustments, see Table \@ref(tab:tabfair).
First, this requires assigning a globally unique and persistent identifier, or DOI, to each theory (F1).
Of the many services that provide DOIs for scientific information artefacts,
Zenodo and the Open Science Framework are commonly used in psychology.
Second, Findable theory is described with rich metadata	(F2).
This includes citation metadata (e.g., referencing a scientific paper that documents the theory, or a psychometric paper that operationalizes specific constructs).
It might further include domain-specific metadata, such as a reference to a taxonomy of psychological constructs [@boscoMetaBUSVehicleFacilitating2017],
ontology [@guyonMeasurementOntologyEpistemology2018],
or catalog of psychological phenomena [REF Noah Denny].
Metadata should also include identifiers for all the versions of the theory it describes	(F3);
Zenodo handles this by default by providing an overarching DOI for an information artifact which subsumes the DOIs of that artifact's versions.
Finally, these metadata should be registered or indexed in a searchable registry (F4).
This final criterion is less straightforward.
Ideally, FAIR theories should be indexed in search engines used by academics, like Google Scholar.
At present, however, these search engines are designed to index traditional print publications.
The *data paper* solves this problem for research data;
the idea is that scholars publish a paper (or even preprint) as documentation for the data resource [REF McGIllivray on data papers].
The data paper is indexed by search engines, and in turn points to the relevant information artifact.
The same solution could be applied to theories - but it seems superfluous to generate papers whose only purpose is to redirect to a specific resource.
Another solution is to manually index FAIR theories,
for example by adding them to one's Google Scholar profile,
or entering them in PURE.
<!-- CJ: Can we guarantee automatically indexing FAIR theory somehow? -->

At present, theories have poor findability, which impedes cumulative knowledge acquisition.
One factor contributing to theories' lack of Findability is the lack of standardized metadata, or even a standardized keyword to signal the presence of theory within a paper - terms like "theory", "model", and "framework" are used interchangeably.
To curb this trend, we suggest using the keyword `"FAIRtheory"` for all resources that constitute or reference a FAIR theory (separating the words `FAIR` and `theory` by a space or hyphen would lead them to be interpreted as separate tokens in many search engines.
This would allow theoretical resources to be systematically indexed, tagged, and made searchable.
Another factor contributing to the present lack of Findability is that the primary unit of dissemination and search in psychology is still the academic paper.
A paper may contain multiple resources - such as materials, data, code, and theory - but if these are not merely described in text, and not instantiated as separate informational artefacts, their findability is limited.
This would be achieved by modular publishing of theories as individually citable academic assets, with adequate metadata that is indexed in standardized repositories,
similar to the current practice of publishing empirical data in standardized repositories (e.g., DataVerse).
As with empirical data, these theories could still be connected to a specific paper which might serve as documentation and the canonical reference for the resource.

There have been notable efforts to improve theories' findability through post-hoc curation.
For example, Gray and colleagues introduced a format for representing theories,
and post many examples on their website [@grayHowMapTheory2017].
Similarly, Borsboom and colleagues seek to establish a database of psychological theories [REF BORSBOOM].
Post-hoc curation is a notable effort but does not address the root cause of the lack of Findability, however.
Ideally, Findability would be addressed ante-hoc, through documentation with rich metadata and modular publishing.

## Accessibility

Transparent scholarly communication about theory requires that theories are accessible to all researchers and other stakeholders.
If theories are accessible, researchers can reuse and refine them,
thus accelerating cumulative knowledge acquisition.
Making theories accessible also allows stakeholders (e.g., practitioners, policy makers, advocates) to inform themselves of the current scientific understanding of specific phenomena.
While isolated empirical findings can appear fragmented and contradictory [@dumas-malletPoorReplicationValidity2017],
theories offer a top-down, big picture representation of the phenomena studied in a field.
In other words, theories are an important instrument in science communication.

The Accessibility criteria serve to *regulate* access, not to maximize it.
These principles apply to theory with minor changes, with the caveat that there might be less of a need to restrict access to theory than there is for (human subjects) data.
Firstly, theory and its associated metadata should be accessible by their identifier using a standardized communications protocol (A1).
This can be achieved, for example, by hosting theory in a version-controlled remote repository (such as git), and archiving that repository on Zenodo for long-term storage.
The resulting resource will then have an identifier (DOI) which allows the theory to be accessed using a standardized communications protocol (download via `https` or `git`).
Secondly (A2), theory metadata should be accessible, even when the theory is no longer available,
which is also achieved via long-term storage (e.g., on Zenodo).
Git remote repositories allow for access control,
and Zenodo allows for access control of individual files/resources.
An unavailable theory typically refers to a theory that was abandoned in favor of a better or more general theory (such as the phlogiston theory, which was superseded by thermodynamics).
In general, it makes sense to keep outdated theories, in order to be able to track the genesis of theories over time, yet, we require the availability of meta data as a minimum requirement.

At present, there are several impediments to theories' accessibility.
To the extent that theories are still contained within papers,
paywalls erected by commercial publishers constitute a barrier.
Open Access publishing thus increases the accessibility of all academic output, including theory.
A second impediment is more indirect:
While open access publishing increases practical access to theories,
accessibility also requires clear and explicit communication.
This property of good theories has been dubbed "discursive survival [...], the ability to be understood" [@guestWhatMakesGood2024].
The current prevalence of strategic ambiguity renders psychological theory difficult to understand [@frankenhuisStrategicAmbiguitySocial2023].
It is important to acknowledge the *indeterminacy of translation* [@quineReasonsIndeterminacyTranslation1970]:
which holds that every communicative utterance has multiple alternative translations, with no *objective* means of choosing the correct one.
It follows that an idea cannot be formalized to the point that it becomes unambiguously interpretable.
This places a theoretical upper bound on theories' ability to be understood.

Successful communication requires shared background knowledge between sender and receiver [@vogtFAIR20Extending2024].
The Kuhnian notion of "normal science", conducted within the context of a shared paradigm, provides shared background knowledge to facilitate mutual understanding [@kuhnStructureScientificRevolutions2009].
From a pragmatic perspective, these considerations indicate that,
when striving to make theory accessible,
it is important to be as explicit as possible (e.g., about assumptions  and ontological definitions),
while acknowledging that accessibility exists on a spectrum,
and that it is impossible to eliminate all ambiguity.
Rather, it may benefit scientific discourse to anticipate misunderstanding,
and use it to drive further explication of theory.
In sum, efforts to communicate theory clearly, with as few dependencies on shared background knowledge as possible, including by formalization, embedding within shared contexts, and explication of assumptions, will advance its Accessibility.

A third impediment arises when theories have a "dependency on the author" (DOA). <!-- AB citation needed?! Who came up with this term? -->
DOA occurs when a theory cannot be understood by independent scholars,
thus requiring the original author for interpretation and clarification.
We have heard DOA referred to apocryphally as the "ask Leon" phenomenon,
as graduate students were supposedly told to ask Leon Festinger to explain to them how their misconstrual of cognitive dissonance theory had caused their experiments to yield null results.
DOA relates to the discourse on "Great Man Theorizing" [@guestWhatMakesGood2024] because it enables gatekeeping: an author could insist that work requires their involvement or denounce work conducted outside their purview as illegitimate,
which violates checks and balances of scientific research.
DOA also renders theories immune to refutation,
because the author can claim that the theory was misconstrued when confronted with falsifying evidence, thus making it a moving target  [@szollosiArrestedTheoryDevelopment2021].
The fact that DOA is inherently problematic is illustrated by cases where third parties identify logical inconsistencies within a theory [e.g., @kissnerIDENTIFICATIONLOGICALINCONSISTENCY2008].
This demonstrates that original authors are not the ultimate authority on their theories.
DOA thus unduly impedes scientific progress, and authors should make good-faith efforts to make theories as accessible as possible; both in terms of availability and in terms of interpretability.

<!-- The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing. -->

## Interoperability

Interoperability pertains to the property of information artefacts to "integrate or work together [...] with minimal effort" [@wilkinsonFAIRGuidingPrinciples2016a].
The original interoperability principles can be rephrased somewhat to apply to theory.
Firstly, theory and its associated metadata should use a formal, accessible, shared and broadly applicable language to facilitate (human- and) machine readability and reuse (I1).
The common practice of instantiating theory as lengthy prose or multi-interpretable bitmap image falls short of this ideal.
Instead, FAIR theory should, ad minimum,
be instantiated as 
as a type of data that is human- and machine-readable with as few interpretative steps as possible,
as previously recommended [@vanlissaWORCSWorkflowOpen2021].
Depending on the level of formalization of the theory,
different formats may be appropriate,
such as verbal statements in plain text,
mathematical formulae, <!-- AB: US readers may expect formulas here ? not sure-->
and statements expressed in some axiomatic system.
Examples of the latter include pseudo-code,
interpretable computer code,
and Gray's theory maps [@grayHowMapTheory2017].
While a theory represented as a bitmap image is not very interoperable,
the same image represented in the DOT language <!--AB citation needed; also Aaron has his own Julia package for graph language specs - not sure whether Aaron wants to refer to this?--> for representing graphs does meet this ideal.

Secondly, theory (meta)data should use vocabularies that follow FAIR principles (I2).
This is essentially a call to establish standardized ontologies,
which are themselves a type of theory [@meehlAppraisingAmendingTheories1990]. 
Thirdly, theory (meta)data should include qualified references to other (meta)data, including previous versions of the theory (I3).
The first part of this principle allows for nested theories;
for example, a theory that specifies causal relationships between constructs could refer back to an ontological theory from which those constructs are derived.
This can be achieved by linking the DOI of those nested theories [@ContributingCitationsReferences].
The second part of this principle allows for tracing the provenance of a theory; keeping track of its prior versions and other theories that inspired it.
This can be achieved by using Git for version control and Zenodo for archiving.

As the original definition of interoperability was somewhat narrow [@wilkinsonFAIRGuidingPrinciples2016a],
the concept has recently been further refined in terms of facilitating "successful communication between machines and between humans and machines", where "A and B are considered X-interoperable if a common operation X exists that can be applied to both" [@vogtFAIR20Extending2024].
This definition invites the question: *interoperable for what?*
Suitable answers for FAIR theory may be:
this theory is X-interoperable for deriving testable hypotheses,
or for the purpose of selecting relevant control variables,
or for the purpose of indicating the conditions necessary for observing a particular phenomenon.
This revised definition implies that theories have specific properties that incur affordances in terms of X-interoperability;
for example, Table \@ref(tab:tabmeehl) illustrates the affordances of Meehl's nine properties of strong theories (properties 3-8 are grouped because they all refer to functional form).

```{r tabmeehl}
data.frame(
  "Property" = c("1) Ontology", "2) Causal connections", "3-8) Functional Form", "9) Numerical Value"),
  "X-interoperability" = c("Variable selection", "Model specification, covariate selection, causal inference", "Deriving specific hypotheses", "Simulating data"), check.names = FALSE
) |>
papaja::apa_table()
```


<!-- Do we want to say something about this?
Interoperability of psychological theory may be limited when theories are instantiated in natural language without consideration of ontological complexities. -->
<!-- or as visualizations without a straightforward interpretation -->
<!-- What can go wrong? A well-formed sentence may be logically inconsistent (e.g., the theory that conceptualized father involvement simultaneously as mediator and moderator); a picture may seem sensible but there's no straightforward way to translate it to hypotheses, etc. -->
<!-- which are not machine-readable and l -->

With regard to the state of interoperability in contemporary psychology, 
Kurt Lewin's adage "there's nothing as practical as a good theory" [@lewinPsychologyProcessGroup1943] implies that ought to be highly X-interoperable in psychological researchers' day-to-day work.
But, as we argued, this is not the case.
The examples of X-interoperability offered in Table \@ref(tab:tabmeehl) illustrate that much can be gained by integrating theory directly into analysis workflows, and by making theory X-interoperable within software used for analysis.
For example, interoperable theory could be used
to select control variables for causal inference [@cinelliCrashCourseGood2022],
or to preregister the inferential procedure that would lead to specific modifications of a theory after analyzing empirical data [@peikertReproducibleResearchTutorial2021],
or to derive machine-readable hypotheses [@lakensImprovingTransparencyFalsifiability2021] which could be automatically evaluated through integration testing [@vanlissaUsingEndpointsCheck2023].
Furthermore, theories can be X-interoperable with each other to enable nesting, or using one theory to clarify elements of another theory.
For example, it should be possible to embed a theory about emotion regulation [e.g., @grossEmotionRegulationCurrent2015] within a theory of emotion regulation development [@morrisRoleFamilyContext2007].

## Reusability

If take cumulative knowledge acquisition to be a goal of scientific research, then Reusability is the ultimate purpose of making theory FAIR.
Applied to FAIR theory, reusability requires that  theory and its associated metadata are richly described with a plurality of accurate and relevant attributes (R1) with a clear and accessible license for reuse (R1.1), detailed provenance (R1.2), and (meta)data which meets domain-relevant community standards (R1.3).
As we will argue below, the most appropriate license for theory reuse is likely to be CC0 (no rights reserved), <!-- AB : maybe add a caveat that there may be country-specific legislation, company property rights, etc. and that this is of course no legal advice-->
although this should be combined with a culture of comprehensive (theory) citation to meet other open science requirements [REF TOP guidelines].
Criterion R1.2 is met by version control with Git and archival on Zenodo.
Domain-relevant community standards, to a large extent, remain to be established - and this paper is the first step towards further work in that area.

If we consider the current state of Reusability in psychological theory, there appears to be a norm *against* theory reuse:
*"[Theories are] like toothbrushes — no self-respecting person wants to use anyone else's"* [@mischelToothbrushProblem2008].
This norm impedes scientific progress.
Cumulative knowledge acquisition requires reusable theories that are continuously updated based on insights from new data [@degrootMethodologieGrondslagenVan1961].
<!-- Theories are made reusable by thorough documentation, -->
<!-- appropriate licensing,  -->
<!-- and detailed provenance (e.g., through version control, cross-referencing, and semantic versioning). -->
In our workshops on FAIR Theory, we similarly notice reluctance to the notion of reusing and adapting theories,
reflected in questions such as "who owns theory",
and "who determines how a theory may be reused or changed"?
These questions imply a norm against modifying theory without consent from the author reminiscent of the aforementioned problem of dependency on the author.

Licensing theories for reuse provides an unambiguous answer to such questions.
In determining what license is appropriate for theory,
it is important to consider that copyright law limits authors' rights based on the *idea-expression dichotomy* [@bently2010copyright],
which holds that copyright explicitly does not
*"extend to any idea, procedure, process, system, method of operation, concept, principle, or discovery"*.
Copyright may, however, extend to creative works expressing that idea (e.g., writing, visual illustrations).
It thus seems that vague, ambiguous verbal explanations of theories - in other words, those that fall short of the Accessibility criterion - are more likely to qualify for copyright protection than formal theories.
If copyright limits Reusability and does not cover ideas in their purest form (like formal theories),
then it might be counterproductive and possibly misleading to adopt a license that assumes copyright protection.
Furthermore, even if copyright would apply, academic research is covered under "fair use" exemptions,
so copyright would pose few restrictions to Reusability in scholarly communication.
Given these considerations, the CC0 (no rights reserved) license seems most appropriate for FAIR Theory;
it explicitly waives all rights and encourages reuse.
In principle, CC0 does not require attribution.
Nevertheless, is essential that scholars do comprehensively cite theory,
including prior work that new theories are based on,
even in absence of legal obligations to do so,
to meet the definition of Reusability (R1.2, Table \@ref(tab:tabfair) and to comply with other definitions of open scholarship [@aalbersbergMakingScienceTransparent2018].

## Additional considerations

We can take inspiration from the field of computer science for well-established processes for iteratively improving information artefacts,  like computer code (which we also have successfully applied in the domain of reproducible research findings, see XXX). <!--AB cite WORCS and Aarons tutorial on how to do the same thing again..-->
Using version control systems, like git, would enhance the reusability of FAIR theory by thoroughly documenting every modification in a traceable and reversible manner.
Git also facilitates diffuse and adversarial collaboration,
as independent researchers can create independent versions of existing theories through "forking", or suggest modifications to existing theories via "pull requests".
In sum, version control using Git enables systematic, collaborative, and transparent theory development,
enables studying the provenance of a theory and investigating how well different iterations of the theory explain empirical evidence [@vanlissaUsingEndpointsCheck2023].

Even if scholars wish to diverge substantially from prior theory,
explicitly referring back to it enables clear comparison of the differences [@ramGitCanFacilitate2013].


From a meta-science perspective, FAIR theory facilitates studying the state of theory in a particular subfield, and comparing theories' substantive and structural properties. 

### Making Theories FAIR Accelerates Scientific Progress
Adopting the FAIR principles for theories can address key challenges in the current research landscape, where theories often remain isolated and underutilized. By making theories findable, accessible, interoperable, and reusable, researchers can ensure that their work is grounded in a shared, transparent, and cumulative body of knowledge. This approach enhances scholarly communication, allowing for greater scrutiny, replication, and collaboration across disciplines, ultimately leading to faster, more reliable, and more impactful scientific progress.



<!-- ## FAIR Theory and Recognition & Rewards -->

<!-- FAIR theory provides a clear deliverable, and a clear goal, for scholars and institutions seeking to promote contributions to theory. -->
<!-- In the spirit of DORA, this helps researchers obtain credit for their theoretical contributions - obviating the necessity of publishing a purely theoretical paper, which can be challenging. -->



# Examples

## Formalizing the Empirical Cycle

In this example, we represent the empirical cycle - a theory of cumulative knowledge production through scientific research - as FAIR theory.
As several authors have taken inspiration from the work by De Groot  [@degrootMethodologieGrondslagenVan1961],  
we compare our interpretation of the original theory to the interpretation of others.
Originally, the theory has the following structure:

```

digraph {

  observation;
  induction;
  deduction;
  test;
  evaluation;
  
  observation -> induction;
  induction -> deduction;
  deduction -> test;
  test -> evaluation;
  evaluation -> observation;
  
}
```

Subsequently, Wagenmakers and colleagues modified the theory by *"[adding the] Whewell-Peirce-Reichenbach distinction between the context of discovery and the context of justification"*:

```
digraph {

  subgraph cluster_discovery {
    label="Discovery";
    hypothesis [label="New hypothesis"];
    prediction [label="New prediction"];
  }
  data  [label="Old knowledge and old data"];      
  subgraph cluster_justification {
    label="Justification";
    test [label="Test on new data"];
    evaluation;
  }

  data -> hypothesis [label="Speculate & explore"];
  hypothesis -> prediction  [label="Deduce"];
  prediction -> test  [label="Design new experiment"];
  test -> evaluation  [label="Statistical analysis"];
  evaluation -> data  [label="Knowledge accumulation"];

}
```

<!--AB TODO: Also add images of the graph syntax - not everyone will be able to read this in syntax and see the graph in their minds-->

Note, however, that there appear to be further changes:
the phases of the cycle have been renamed,
and the annotations suggest a move towards experimental empirical psychology that was absent in the original formulation.
Moreover, the label "knowledge accumulation" invites the question of exactly *how* knowledge accumulates upon evaluation of a prior experiment.
As this lack of cumulative knowledge acquisition appears to be precisely where contemporary research practice falls short, this ambiguity invites further improvement of the theory.

Our work, too is inspired by De Groot, but our take on the empirical cycle is different again:

```
digraph {

  theory;
  prediction;
  test [label="inferential procedure"];
  observation;
  
  theory -> prediction [label="deduction"];
  prediction -> test;
  test -> observation;
  observation -> theory [label="generalization"];

}
```

In our representation,
induction is not a separate phase but a mode of reasoning by which specific observations are generalized into theory.
For example, the refutation of a hypothesized effect,
or the serendipitous observation of some pattern in data,
might be a reason to revise or construct theory.
Induction, incidentally, also occurs within the link from prediction to testing:
in the form of the inductive bias of methods used to perform the test,
and auxiliary assumptions that must be made to address remaining theoretical ambiguities.

<!-- # And then for example: I understand Aaron's work about inductive bias to be about the link "prediction -> test;", because the test is not identified without making auxiliary assumptions; if the auxiliary assumption is made based on the data, inductive bias is introduced. -->
<!-- ``` -->


## Using FAIR Theory to Perform Causal Inference

Some have argued that *causal explanations* are a property of good theory [REF Meehl, etc?].
According to Pearl and colleagues,
explicit assumptions about the direction of causality allow one to perform causal inference even on cross-sectional data.
Any formal theory that is explicit about direction of causality could thus be used to guide causal inference,
and could even be integrated into the analysis environment.

In this example, we illustrate how to use DAGs for causal inference, including the detection of a violation of the initial model and subsequent adaptation of the DAG. We could use that to illustrate updating FAIR theory:

https://currentprotocols.onlinelibrary.wiley.com/doi/full/10.1002/cpz1.45

We can find more examples of causal inference with DAGs in these tutorials:

https://www.r-bloggers.com/2019/08/causal-inference-with-dags-in-r/

https://www.r-bloggers.com/2018/08/applications-of-dags-in-causal-inference/


<!-- NOTES -->

* Theory is the vehicle of cumulative knowledge acquisition
* According to the empirical cycle, ideally, hypotheses are derived from theory, then tested in data, and theory is amended based on the resulting insights. When this cycle is regularly completed, theories become ever more veracious representations of social scientific phenomena.
* At present, there is concern over a theory crisis in the social sciences, which highlights that this system is not functioning as intended, and highlights the need for better theory.
* One source of potential improvements of theory methodology that has not been previously considered is computer science.
* The process of "iteratively improving" digital objects - in this case, computer code - is well understood.
* Recent work like the FAIR software principles has demonstrated that ideals of open science apply to computer science as well.
* This paper argues that, conversely, principles of computer science - particularly version control, algorithmic hypothesis generation (find better word; this is about using the digital theory object to derive implied hypotheses), and integrated testing, can also be used to improve theory methods in the social science.
* We introduce "FAIR theory", a digital research artifact to represent formal social scientific theories
* FAIR theory can be version controlled; any time new insights require modifications of the theory, these modifications can be documented in a traceable and reversable manner. Version control also enables diffuse collaboration in theory development, as other researchers can submit "pull requests" to suggest modifications of a theory, or can "fork" existing theories to create a spin-off from an existing theory.
* FAIR theory allows for algorithmic derivation of hypotheses implied by the theory.
* FAIR theory enables integration testing: researchers can build a "test suite" of evidence that must be explainable by the theory, and any modifications of the theory must also pass the test suite.
* To illustrate FAIR theory's potential to accelerate cumulative knowledge acquisition, we present several tutorial examples, developed in collaboration with applied researchers across fields of social science.

# Discussion

## Future Directions

One remaining issue that intersects with FAIR Theory is the measurement and operationalization of psychological constructs.
Aside from the aforementioned "theory crisis", there has been talk of a "measurement crisis":  <!--AB very important point that I was already missing in the introduction; seems we also need FAIR measurements :) -->
it is not always clear how theoretical constructs are operationalized, and many existing instruments have poor psychometric properties [REF].
Additionally, the "jingle-jangle" fallacy is prevalent in the social sciences:
the same term is often used for distinct constructs, and conversely, different terms are used to refer to the same construct.
FAIR Theory can help address the measurement crisis:
since theories can reference other theories and resources, it is possible to extend a structural theory with a theory of 

FAIR Theory incorporates theory into open science workflows,
facilitates scholarly communication about theories,
making it easier to share theories with less opportunity for ambiguity and misunderstanding.
FAIR Theories are easier to find, and facilitate sharing, reusing, and updating open theories.
More efficient and transparent communication about theory democratizes and accelerates cumulative knowledge acquisition,
removes barriers for knowledge exchange with the global scholarly community,
opens theory development to diverse perspectives, and enables (distributed and adversarial) collaboration.



\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
