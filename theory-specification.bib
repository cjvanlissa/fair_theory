@article{aalbersbergMakingScienceTransparent2018,
  title = {Making {{Science Transparent By Default}}; {{Introducing}} the {{TOP Statement}}},
  author = {family=Aalbersberg, given=IJsbrand Jan, given-i={{IJ}}J and Appleyard, Tom and Brookhart, Sarah and Carpenter, Todd and Clarke, Michael and Curry, Stephen and Dahl, Josh and DeHaven, Alexander Carl and Eich, Eric and Franko, Maryrose and Freedman, Len and Graf, Chris and Grant, Sean and Hanson, Brooks and Joseph, Heather and Kiermer, Veronique and Kramer, Bianca and Kraut, Alan and Karn, Roshan Kumar and Lee, Carole and MacFarlane, Aki and Martone, Maryann and Mayo-Wilson, Evan and McNutt, Marcia and McPhail, Meredith and Mellor, David Thomas and Moher, David and Mudditt, Alison and Nosek, Brian A. and Orland, Belinda and Parker, Timothy H. and Parsons, Mark and Patterson, Mark and Santos, Solange and Shore, Carolyn and Simons, Daniel J. and Spellman, Bobbie and Spies, Jeffrey Robert and Spitzer, Matthew and Stodden, Victoria and Swaminathan, Sowmya and Sweet, Deborah and Tsui, Anne and Vazire, Simine},
  date = {2018-02-15},
  doi = {10.31219/osf.io/sm78t},
  url = {https://osf.io/sm78t},
  urldate = {2020-01-08},
  abstract = {In order to increase the replicability of scientific work, the scientific community has called for practices designed to increase the transparency of research (McNutt, 2014; Nosek et al., 2015). The validity of a scientific claim depends not on the reputation of those making the claim, the venue in which the claim is made, or the novelty of the result, but rather on the empirical evidence provided by the underlying data and methods. Proper evaluation of  the merits of scientific findings requires availability of the methods, materials, and data and the reasoned argument that serve as the basis for the published conclusions (Claerbout and Karrenbach 1992; Donoho et al 2009; Stodden et al 2013; Borwein et al 2013; Munafò et al, 2017). Wide and growing support for these principles (see, for example, signatories to Declaration on Research Assessment, DORA, https://sfdora.org/, and the Transparency and Openness Promotion Guidelines https://cos.io/our-services/top-guidelines/) must be coupled with guidelines to increase open sharing of data and research materials, use of reporting guidelines, preregistration, and replication. We propose that, going forward, authors of all scientific articles disclose the availability and location of all research items, including data, materials, and code, related to their published articles in what we will refer to as a TOP Statement.}
}

@book{bently2010copyright,
  title = {Copyright and {{Piracy}}: {{An}} Interdisciplinary Critique},
  author = {Bently, Lionel and Davis, Jennifer and Ginsburg, Jane C},
  date = {2010},
  volume = {13},
  publisher = {Cambridge University Press}
}

@article{boscoMetaBUSVehicleFacilitating2017,
  title = {{{MetaBUS}} as a Vehicle for Facilitating Meta-Analysis},
  author = {Bosco, Frank A. and Uggerslev, Krista L. and Steel, Piers},
  date = {2017-03-01},
  journaltitle = {Human Resource Management Review},
  shortjournal = {Human Resource Management Review},
  series = {Using {{Meta-analysis}} to {{Enhance}} Our {{Understanding}} of {{Human Resource Management}}},
  volume = {27},
  number = {1},
  pages = {237--254},
  issn = {1053-4822},
  doi = {10.1016/j.hrmr.2016.09.013},
  url = {https://www.sciencedirect.com/science/article/pii/S1053482216300675},
  urldate = {2024-12-05},
  abstract = {To address new research questions and get a clearer picture of research, scientists and practitioners in human resource management have come to rely heavily on meta-analyses. However, meta-analyses may take months or years to produce and are becoming increasingly difficult to produce as the corpus of available research grows exponentially. We describe how the metaBUS platform can assist in tackling two central challenges to conducting meta-analyses. In addition, we provide a detailed description of the platform, with information on all fields included in the database. Next, we provide recommendations for three use cases: generating literature search terms by using the metaBUS taxonomy, conducting metaBUS queries to locate findings and generate first-pass meta-analyses, and identifying relevant findings that might have gone overlooked during traditional literature searches. We demonstrate a new software and a cloud-based interface that allow users to leverage the platform. We conclude with implications, limitations, and future directions.},
  keywords = {Big data,Literature search,Meta-analysis quantitative synthesis},
  file = {C:\Users\vanlissa\Zotero\storage\VPC3KBV4\S1053482216300675.html}
}

@article{cinelliCrashCourseGood2022,
  title = {A {{Crash Course}} in {{Good}} and {{Bad Controls}}},
  author = {Cinelli, Carlos and Forney, Andrew and Pearl, Judea},
  date = {2022-05-20},
  journaltitle = {Sociological Methods \& Research},
  pages = {00491241221099552},
  publisher = {SAGE Publications Inc},
  issn = {0049-1241},
  doi = {10.1177/00491241221099552},
  url = {https://journals.sagepub.com/doi/full/10.1177/00491241221099552},
  urldate = {2024-03-12},
  abstract = {Many students of statistics and econometrics express frustration with the way a problem known as “bad control” is treated in the traditional literature. The issue arises when the addition of a variable to a regression equation produces an unintended discrepancy between the regression coefficient and the effect that the coefficient is intended to represent. Avoiding such discrepancies presents a challenge to all analysts in the data intensive sciences. This note describes graphical tools for understanding, visualizing, and resolving the problem through a series of illustrative examples. By making this “crash course” accessible to instructors and practitioners, we hope to avail these tools to a broader community of scientists concerned with the causal interpretation of regression models.},
  keywords = {back-door criterion,bad controls,causal inference,DAG,regression}
}

@online{ContributingCitationsReferences,
  title = {Contributing {{Citations}} and {{References}}},
  url = {https://support.datacite.org/docs/data-citation},
  urldate = {2024-12-05},
  abstract = {Citations and references can be created by adding relatedIdentifiers to DataCite DOI metadata. Each relatedIdentifier has a required relationType attribute which is used to denote the type of relationship. You can add citations and references to DOI metadata when you create the DOI initially and wit...},
  langid = {english},
  organization = {DataCite Support},
  file = {C:\Users\vanlissa\Zotero\storage\PUMBH9TE\contributing-citations-and-references.html}
}

@book{degrootMethodologieGrondslagenVan1961,
  title = {Methodologie: Grondslagen van onderzoek en denken in de gedragswetenschappen},
  shorttitle = {Methodologie},
  author = {family=Groot, given=Adriaan D., prefix=de, useprefix=true},
  date = {1961},
  eprint = {6hiBDwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {Uitgeverij Mouton},
  location = {'s Gravenhage},
  isbn = {90-279-7721-6},
  langid = {dutch},
  pagetotal = {440},
  keywords = {Language Arts & Disciplines / Linguistics / General}
}

@book{degrootMethodologyFoundationsInference1969,
  title = {Methodology: {{Foundations}} of Inference and Research in the Behavioral Sciences},
  shorttitle = {Methodology},
  author = {De Groot, Adriaan D. and Spiekerman, J. A. A.},
  date = {1969-01-01},
  publisher = {De Gruyter Mouton},
  doi = {10.1515/9783112313121},
  url = {https://www.degruyter.com/view/title/571174},
  urldate = {2022-02-06},
  isbn = {978-3-11-230199-9 978-3-11-231312-1},
  file = {C:\Users\vanlissa\Zotero\storage\KCB5FJJR\De Groot and Spiekerman - 1969 - Methodology Foundations of inference and research.pdf}
}

@article{dumas-malletPoorReplicationValidity2017,
  title = {Poor Replication Validity of Biomedical Association Studies Reported by Newspapers},
  author = {Dumas-Mallet, Estelle and Smith, Andy and Boraud, Thomas and Gonon, François},
  date = {2017-02-21},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {12},
  number = {2},
  pages = {e0172650},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0172650},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0172650},
  urldate = {2024-02-27},
  abstract = {Objective To investigate the replication validity of biomedical association studies covered by newspapers. Methods We used a database of 4723 primary studies included in 306 meta-analysis articles. These studies associated a risk factor with a disease in three biomedical domains, psychiatry, neurology and four somatic diseases. They were classified into a lifestyle category (e.g. smoking) and a non-lifestyle category (e.g. genetic risk). Using the database Dow Jones Factiva, we investigated the newspaper coverage of each study. Their replication validity was assessed using a comparison with their corresponding meta-analyses. Results Among the 5029 articles of our database, 156 primary studies (of which 63 were lifestyle studies) and 5 meta-analysis articles were reported in 1561 newspaper articles. The percentage of covered studies and the number of newspaper articles per study strongly increased with the impact factor of the journal that published each scientific study. Newspapers almost equally covered initial (5/39 12.8\%) and subsequent (58/600 9.7\%) lifestyle studies. In contrast, initial non-lifestyle studies were covered more often (48/366 13.1\%) than subsequent ones (45/3718 1.2\%). Newspapers never covered initial studies reporting null findings and rarely reported subsequent null observations. Only 48.7\% of the 156 studies reported by newspapers were confirmed by the corresponding meta-analyses. Initial non-lifestyle studies were less often confirmed (16/48) than subsequent ones (29/45) and than lifestyle studies (31/63). Psychiatric studies covered by newspapers were less often confirmed (10/38) than the neurological (26/41) or somatic (40/77) ones. This is correlated to an even larger coverage of initial studies in psychiatry. Whereas 234 newspaper articles covered the 35 initial studies that were later disconfirmed, only four press articles covered a subsequent null finding and mentioned the refutation of an initial claim. Conclusion Journalists preferentially cover initial findings although they are often contradicted by meta-analyses and rarely inform the public when they are disconfirmed.},
  langid = {english},
  keywords = {ADHD,Bibliometrics,Biomarkers,Cancer risk factors,Medical risk factors,Mental health and psychiatry,Metaanalysis,Scientific publishing},
  file = {C:\Users\vanlissa\Zotero\storage\UWPM7YLC\Dumas-Mallet et al. - 2017 - Poor replication validity of biomedical associatio.pdf}
}

@article{frankenhuisStrategicAmbiguitySocial2023,
  title = {Strategic {{Ambiguity}} in the {{Social Sciences}}},
  author = {Frankenhuis, Willem E. and Panchanathan, Karthik and Smaldino, Paul E.},
  date = {2023-11-17},
  journaltitle = {Social Psychological Bulletin},
  volume = {18},
  pages = {1--25},
  issn = {2569-653X},
  doi = {10.32872/spb.9923},
  url = {https://spb.psychopen.eu/index.php/spb/article/view/9923},
  urldate = {2024-03-11},
  abstract = {In the wake of the replication crisis, there have been calls to increase the clarity and precision of theory in the social sciences. Here, we argue that the effects of these calls may be limited due to incentives favoring ambiguous theory. Intentionally or not, scientists can exploit theoretical ambiguities to make support for a claim appear stronger than it is. Practices include theory stretching, interpreting an ambiguous claim more expansively to absorb data outside of the scope of the original claim, and post-hoc precision, interpreting an ambiguous claim more narrowly so it appears more precisely aligned with the data. These practices lead to the overestimation of evidence for the original claim and create the appearance of consistent support and progressive research programs, which may in turn be rewarded by journals, funding agencies, and hiring committees. Selection for ambiguous research can occur even when scientists act in good faith. Although ambiguity might be inevitable or even useful in the early stages of theory construction, scientists should aim for increased clarity as knowledge advances. Science benefits from transparently communicating about known ambiguities. To attain transparency about ambiguity, we provide a set of recommendations for authors, reviewers, and journals. We conclude with suggestions for research on how scientists use strategic ambiguity to advance their careers and the ways in which norms, incentives, and practices favor strategic ambiguity. Our paper ends with a simple mathematical model exploring the conditions in which high-ambiguity theories are favored over low-ambiguity theories, providing a basis for future analyses.},
  langid = {english},
  keywords = {formal modeling,incentive structures,post-hoc precision,RAPPing,strategic ambiguity,theory development,theory stretching},
  file = {C:\Users\vanlissa\Zotero\storage\PE7K42NU\Frankenhuis et al. - 2023 - Strategic Ambiguity in the Social Sciences.pdf}
}

@article{friedTheoriesModelsWhat2020,
  title = {Theories and {{Models}}: {{What They Are}}, {{What They Are}} for, and {{What They Are About}}},
  author = {Fried, Eiko I.},
  date = {2020-10-01},
  journaltitle = {Psychological Inquiry},
  volume = {31},
  number = {4},
  pages = {336--344},
  publisher = {Routledge},
  issn = {1047-840X},
  doi = {10.1080/1047840X.2020.1854011},
  file = {C:\Users\vanlissa\Zotero\storage\JTLH8EGQ\Fried - 2020 - .pdf}
}

@incollection{gigerenzerNullRitualWhat2004,
  title = {The Null Ritual : {{What}} You Always Wanted to Know about Significance Testing but Were Afraid to Ask},
  shorttitle = {The Null Ritual},
  booktitle = {The {{Sage}} Handbook of Quantitative Methodology for the Social Sciences},
  author = {Gigerenzer, Gerd and Krauss, Stefan and Vitouch, Oliver},
  editor = {Kaplan, David},
  date = {2004},
  pages = {391--408},
  publisher = {Sage},
  location = {Thousand Oaks},
  isbn = {978-0-7619-2359-6},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\AW4645E4\GG_Null_2004.pdf}
}

@article{grayHowMapTheory2017,
  title = {How to {{Map Theory}}: {{Reliable Methods Are Fruitless Without Rigorous Theory}}},
  shorttitle = {How to {{Map Theory}}},
  author = {Gray, Kurt},
  date = {2017-09},
  journaltitle = {Perspectives on Psychological Science},
  volume = {12},
  number = {5},
  pages = {731--741},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691617691949},
  url = {http://journals.sagepub.com/doi/10.1177/1745691617691949},
  urldate = {2019-01-06},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\ZFYUID8A\Gray_2017_How to Map Theory.pdf}
}

@article{grossEmotionRegulationCurrent2015,
  title = {Emotion Regulation: {{Current}} Status and Future Prospects},
  shorttitle = {Emotion {{Regulation}}},
  author = {Gross, James J.},
  date = {2015-01-02},
  journaltitle = {Psychological Inquiry},
  volume = {26},
  number = {1},
  pages = {1--26},
  issn = {1047-840X, 1532-7965},
  doi = {10.1080/1047840X.2014.940781},
  url = {http://www.tandfonline.com/doi/abs/10.1080/1047840X.2014.940781},
  urldate = {2017-10-30},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\QAQAU4DD\gross2015.pdf}
}

@article{guestHowComputationalModeling2021,
  title = {How {{Computational Modeling Can Force Theory Building}} in {{Psychological Science}}},
  author = {Guest, Olivia and Martin, Andrea E.},
  date = {2021-07-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {16},
  number = {4},
  pages = {789--802},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620970585},
  url = {https://doi.org/10.1177/1745691620970585},
  urldate = {2022-10-18},
  abstract = {Psychology endeavors to develop theories of human capacities and behaviors on the basis of a variety of methodologies and dependent measures. We argue that one of the most divisive factors in psychological science is whether researchers choose to use computational modeling of theories (over and above data) during the scientific-inference process. Modeling is undervalued yet holds promise for advancing psychological science. The inherent demands of computational modeling guide us toward better science by forcing us to conceptually analyze, specify, and formalize intuitions that otherwise remain unexamined?what we dub open theory. Constraining our inference process through modeling enables us to build explanatory and predictive theories. Here, we present scientific inference in psychology as a path function in which each step shapes the next. Computational modeling can constrain these steps, thus advancing scientific inference over and above the stewardship of experimental practice (e.g., preregistration). If psychology continues to eschew computational modeling, we predict more replicability crises and persistent failure at coherent theory building. This is because without formal modeling we lack open and transparent theorizing. We also explain how to formalize, specify, and implement a computational model, emphasizing that the advantages of modeling can be achieved by anyone with benefit to all.},
  file = {C:\Users\vanlissa\Zotero\storage\YZQCD2RP\Guest and Martin - 2021 - How Computational Modeling Can Force Theory Buildi.pdf}
}

@article{guestWhatMakesGood2024,
  title = {What {{Makes}} a {{Good Theory}}, and {{How Do We Make}} a {{Theory Good}}?},
  author = {Guest, Olivia},
  date = {2024-01-24},
  journaltitle = {Computational Brain \& Behavior},
  shortjournal = {Comput Brain Behav},
  issn = {2522-087X},
  doi = {10.1007/s42113-023-00193-2},
  url = {https://doi.org/10.1007/s42113-023-00193-2},
  urldate = {2024-05-08},
  abstract = {I present an ontology of criteria for evaluating theory to answer the titular question from the perspective of a scientist practitioner. Set inside a formal account of our adjudication over theories, a metatheoretical calculus, this ontology comprises the following: (a) metaphysical commitment, the need to highlight what parts of theory are not under investigation, but are assumed, asserted, or essential; (b) discursive survival, the ability to be understood by interested non-bad actors, to withstand scrutiny within the intended (sub)field(s), and to negotiate the dialectical landscape thereof; (c) empirical interface, the potential to explicate the relationship between theory and observation, i.e., how observations relate to, and affect, theory and vice versa; (d) minimising harm, the reckoning with how theory is forged in a fire of historical, if not ongoing, abuses—from past crimes against humanity, to current exploitation, turbocharged or hyped by machine learning, to historical and present internal academic marginalisation. This work hopes to serve as a possible beginning for scientists who want to examine the properties and characteristics of theories, to propose additional virtues and vices, and to engage in further dialogue. Finally, I appeal to practitioners to iterate frequently over such criteria, by building and sharing the metatheoretical calculi used to adjudicate over theories.},
  langid = {english},
  keywords = {Metascience,Metatheoretical calculcus,Metatheory,Theoretical virtue,Theory},
  file = {C:\Users\vanlissa\Zotero\storage\6I8JTYDS\Guest - 2024 - What Makes a Good Theory, and How Do We Make a The.pdf}
}

@article{guyonMeasurementOntologyEpistemology2018,
  title = {Measurement, Ontology, and Epistemology: {{Psychology}} Needs Pragmatism-Realism},
  shorttitle = {Measurement, Ontology, and Epistemology},
  author = {Guyon, Hervé and Kop, Jean-Luc and Juhel, Jacques and Falissard, Bruno},
  date = {2018-04-01},
  journaltitle = {Theory \& Psychology},
  volume = {28},
  number = {2},
  pages = {149--171},
  publisher = {SAGE Publications Ltd},
  issn = {0959-3543},
  doi = {10.1177/0959354318761606},
  url = {https://doi.org/10.1177/0959354318761606},
  urldate = {2024-12-05},
  abstract = {Measurement in psychology is at the heart of a major debate in the academic literature. We aim to contribute to a critical discussion of this issue. We propose to reposition the object of this type of measure, namely a mental attribute as measured by mental tests. Mental attributes should be considered not as a true object independent of the knower, but as an emergent property of a person dependent on the social context. On the basis of this clarified ontology, we consider that an empirical approach to measuring a mental attribute is possible. This approach must be resolutely pragmatist and realist. In practical terms, this means that a test needs to be renegotiated relative to the context. The validation of quantitative measures requires verification of a certain number of criteria. Consequently, our work critically explores measures as they are usually implemented in the area of psychometrics.},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\RTRDGCGE\Guyon et al. - 2018 - Measurement, ontology, and epistemology Psycholog.pdf}
}

@article{kissnerIDENTIFICATIONLOGICALINCONSISTENCY2008,
  title = {{{ON THE IDENTIFICATION OF A LOGICAL INCONSISTENCY IN THE GENERAL THEORY OF CRIME}}},
  author = {Kissner, Jason},
  date = {2008-01-01},
  journaltitle = {Journal of Crime and Justice},
  publisher = {Taylor \& Francis Group},
  issn = {0735-648X},
  url = {https://www.tandfonline.com/doi/abs/10.1080/0735648X.2008.9721251},
  urldate = {2024-11-21},
  abstract = {Gottfredson and Hirschi's general theory of crime is widely tested and commented upon and just as widely misapprehended. Criminologists have paid insufficient attention to the implications of the...},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\QJRVQXLW\0735648X.2008.html}
}

@article{kuhbergerPublicationBiasPsychology2014,
  title = {Publication {{Bias}} in {{Psychology}}: {{A Diagnosis Based}} on the {{Correlation}} between {{Effect Size}} and {{Sample Size}}},
  shorttitle = {Publication {{Bias}} in {{Psychology}}},
  author = {Kühberger, Anton and Fritz, Astrid and Scherndl, Thomas},
  date = {2014-09-05},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS One},
  volume = {9},
  number = {9},
  eprint = {25192357},
  eprinttype = {pmid},
  pages = {e105825},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0105825},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4156299/},
  urldate = {2024-03-09},
  abstract = {Background The p value obtained from a significance test provides no information about the magnitude or importance of the underlying phenomenon. Therefore, additional reporting of effect size is often recommended. Effect sizes are theoretically independent from sample size. Yet this may not hold true empirically: non-independence could indicate publication bias. Methods We investigate whether effect size is independent from sample size in psychological research. We randomly sampled 1,000 psychological articles from all areas of psychological research. We extracted p values, effect sizes, and sample sizes of all empirical papers, and calculated the correlation between effect size and sample size, and investigated the distribution of p values. Results We found a negative correlation of r{$\mkern1mu$}={$\mkern1mu$}−.45 [95\% CI: −.53; −.35] between effect size and sample size. In addition, we found an inordinately high number of p values just passing the boundary of significance. Additional data showed that neither implicit nor explicit power analysis could account for this pattern of findings. Conclusion The negative correlation between effect size and samples size, and the biased distribution of p values indicate pervasive publication bias in the entire field of psychology.},
  pmcid = {PMC4156299},
  file = {C:\Users\vanlissa\Zotero\storage\JMJ6JSQX\Kühberger et al. - 2014 - Publication Bias in Psychology A Diagnosis Based .pdf}
}

@book{kuhnStructureScientificRevolutions2009,
  title = {The Structure of Scientific Revolutions},
  author = {Kuhn, Thomas S.},
  date = {2009},
  edition = {3. ed., [Nachdr.]},
  publisher = {Univ. of Chicago Press},
  location = {Chicago},
  isbn = {978-0-226-45807-6 978-0-226-45808-3},
  langid = {english},
  pagetotal = {212},
  file = {C:\Users\vanlissa\Zotero\storage\2AJXKFEE\Kuhn - 2009 - The structure of scientific revolutions.pdf}
}

@incollection{lakatosHistoryScienceIts1971,
  title = {History of {{Science}} and Its {{Rational Reconstructions}}},
  booktitle = {{{PSA}} 1970: {{In Memory}} of {{Rudolf Carnap Proceedings}} of the 1970 {{Biennial Meeting Philosophy}} of {{Science Association}}},
  author = {Lakatos, Imre},
  editor = {Buck, Roger C. and Cohen, Robert S.},
  date = {1971},
  series = {Boston {{Studies}} in the {{Philosophy}} of {{Science}}},
  pages = {91--136},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-94-010-3142-4_7},
  url = {https://doi.org/10.1007/978-94-010-3142-4_7},
  urldate = {2024-02-08},
  abstract = {“Philosophy of science without history of science is empty; history of science without philosophy of science is blind”. Taking its cue from this paraphrase of Kant’s famous dictum, this paper intends to explain how the historiography of science should learn from the philosophy of science and vice versa. It will be argued that (a) philosophy of science provides normative methodologies in terms of which the historian reconstructs ‘internal history’ and thereby provides a rational explanation of the growth of objective knowledge; (b) two competing methodologies can be evaluated with the help of (normatively interpreted) history; (c) any rational reconstruction of history needs to be supplemented by an empirical (socio-psychological) ‘external history’.},
  isbn = {978-94-010-3142-4},
  langid = {english},
  keywords = {Actual History,Demarcation Criterion,Inductive Generalisation,Rational Reconstruction,Rationality Theory},
  file = {C:\Users\vanlissa\Zotero\storage\K7SMK27H\Lakatos - HISTORY OF SCIENCE AND ITS RATIONAL RECONSTRUCTION.pdf}
}

@article{lakensImprovingTransparencyFalsifiability2021,
  title = {Improving {{Transparency}}, {{Falsifiability}}, and {{Rigor}} by {{Making Hypothesis Tests Machine-Readable}}},
  author = {Lakens, Daniël and DeBruine, Lisa M.},
  date = {2021-04-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {2515245920970949},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920970949},
  url = {https://doi.org/10.1177/2515245920970949},
  urldate = {2022-09-20},
  abstract = {Making scientific information machine-readable greatly facilitates its reuse. Many scientific articles have the goal to test a hypothesis, so making the tests of statistical predictions easier to find and access could be very beneficial. We propose an approach that can be used to make hypothesis tests machine-readable. We believe there are two benefits to specifying a hypothesis test in such a way that a computer can evaluate whether the statistical prediction is corroborated or not. First, hypothesis tests become more transparent, falsifiable, and rigorous. Second, scientists benefit if information related to hypothesis tests in scientific articles is easily findable and reusable, for example, to perform meta-analyses, conduct peer review, and examine metascientific research questions. We examine what a machine-readable hypothesis test should look like and demonstrate the feasibility of machine-readable hypothesis tests in a real-life example using the fully operational prototype R package scienceverse.},
  file = {C:\Users\vanlissa\Zotero\storage\PDQXJKL2\Lakens and DeBruine - 2021 - Improving Transparency, Falsifiability, and Rigor .pdf}
}

@article{lamprechtFAIRPrinciplesResearch2019,
  title = {Towards {{FAIR}} Principles for Research Software},
  author = {Lamprecht, Anna-Lena and Garcia, Leyla and Kuzak, Mateusz and Martinez, Carlos and Arcila, Ricardo and Martin Del Pico, Eva and Dominguez Del Angel, Victoria and family=Sandt, given=Stephanie, prefix=van de, useprefix=true and Ison, Jon and Martinez, Paula Andrea and McQuilton, Peter and Valencia, Alfonso and Harrow, Jennifer and Psomopoulos, Fotis and Gelpi, Josep Ll. and Chue Hong, Neil and Goble, Carole and Capella-Gutierrez, Salvador},
  editor = {Groth, Paul},
  date = {2019-11-13},
  journaltitle = {Data Science},
  shortjournal = {DS},
  pages = {1--23},
  issn = {24518492, 24518484},
  doi = {10.3233/DS-190026},
  url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/DS-190026},
  urldate = {2020-05-19},
  file = {C:\Users\vanlissa\Zotero\storage\4YVQJARQ\Lamprecht et al_2019_Towards FAIR principles for research software.pdf}
}

@article{lavelleWhenCrisisBecomes2021,
  title = {When a {{Crisis Becomes}} an {{Opportunity}}: {{The Role}} of {{Replications}} in {{Making Better Theories}}},
  shorttitle = {When a {{Crisis Becomes}} an {{Opportunity}}},
  author = {Lavelle, Jane Suilin},
  date = {2021-04-14},
  journaltitle = {The British Journal for the Philosophy of Science},
  shortjournal = {The British Journal for the Philosophy of Science},
  pages = {714812},
  issn = {0007-0882, 1464-3537},
  doi = {10.1086/714812},
  url = {https://www.journals.uchicago.edu/doi/10.1086/714812},
  urldate = {2022-03-01},
  abstract = {While it is widely acknowledged that psychology is in the throes of a replication ‘crisis’, relatively little attention has been paid to the role theory plays in our evaluation of replications as ‘failed’ or ‘successful’. This paper applies well-known arguments in philosophy of science about the interplay between theory and experiment to a contemporary case study of infants’ understanding of false belief (Onishi and Baillargeon [2005]), and attempts to replicate it. It argues that the lack of consensus about over-arching theories informing both the concepts under study and the methodologies used to track them means that researchers disagree over which experiments constitute replications of the original. The second part of the paper places this specific debate within a broader discussion of the replication crisis as a crisis of ‘theory’, developing work by Muthukrishna and Henrich ([2018]) and Bird ([2018]). Bird argues that the lack of agreed over-arching theories in psychology means that a high rate of replication failure is to be expected; this paper agrees with his diagnosis but challenges his proposal that more replication will resolve the problem.},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\XDG74HWE\Lavelle - 2021 - When a Crisis Becomes an Opportunity The Role of .pdf}
}

@book{lewandowsky2010computational,
  title = {Computational Modeling in Cognition: {{Principles}} and Practice},
  author = {Lewandowsky, Stephan and Farrell, Simon},
  date = {2010},
  publisher = {Sage}
}

@article{lewinPsychologyProcessGroup1943,
  title = {Psychology and the {{Process}} of {{Group Living}}},
  author = {Lewin, Kurt},
  date = {1943-02},
  journaltitle = {The Journal of Social Psychology},
  shortjournal = {The Journal of Social Psychology},
  volume = {17},
  number = {1},
  pages = {113--131},
  issn = {0022-4545, 1940-1183},
  doi = {10.1080/00224545.1943.9712269},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00224545.1943.9712269},
  urldate = {2024-11-20},
  langid = {english}
}

@article{mcphetresDecadeTheoryReflected2021,
  title = {A Decade of Theory as Reflected in {{Psychological Science}} (2009–2019)},
  author = {McPhetres, Jonathon and Albayrak-Aydemir, Nihan and Mendes, Ana Barbosa and Chow, Elvina C. and Gonzalez-Marquez, Patricio and Loukras, Erin and Maus, Annika and O’Mahony, Aoife and Pomareda, Christina and Primbs, Maximilian A. and Sackman, Shalaine L. and Smithson, Conor J. R. and Volodko, Kirill},
  date = {2021-03-05},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {16},
  number = {3},
  pages = {e0247986},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0247986},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0247986},
  urldate = {2022-03-09},
  abstract = {The dominant belief is that science progresses by testing theories and moving towards theoretical consensus. While it’s implicitly assumed that psychology operates in this manner, critical discussions claim that the field suffers from a lack of cumulative theory. To examine this paradox, we analysed research published in Psychological Science from 2009–2019 (N = 2,225). We found mention of 359 theories in-text, most were referred to only once. Only 53.66\% of all manuscripts included the word theory, and only 15.33\% explicitly claimed to test predictions derived from theories. We interpret this to suggest that the majority of research published in this flagship journal is not driven by theory, nor can it be contributing to cumulative theory building. These data provide insight into the kinds of research psychologists are conducting and raises questions about the role of theory in the psychological sciences.},
  langid = {english},
  keywords = {Cognitive psychology,Experimental psychology,Psychologists,Psychology,Reaction time,Scientific publishing,Scientists,Semantics},
  file = {C\:\\Users\\vanlissa\\Zotero\\storage\\DZ9FJXZN\\McPhetres et al_2021_A decade of theory as reflected in Psychological Science (2009–2019).pdf;C\:\\Users\\vanlissa\\Zotero\\storage\\ETP3X67C\\article.html}
}

@article{meehlAppraisingAmendingTheories1990,
  title = {Appraising and {{Amending Theories}}: {{The Strategy}} of {{Lakatosian Defense}} and {{Two Principles}} That {{Warrant It}}},
  shorttitle = {Appraising and {{Amending Theories}}},
  author = {Meehl, Paul E.},
  date = {1990-04},
  journaltitle = {Psychological Inquiry},
  shortjournal = {Psychological Inquiry},
  volume = {1},
  number = {2},
  pages = {108--141},
  issn = {1047-840X, 1532-7965},
  doi = {10.1207/s15327965pli0102_1},
  url = {http://www.tandfonline.com/doi/abs/10.1207/s15327965pli0102_1},
  urldate = {2022-05-02},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\5CMGPZTH\Meehl - 1990 - Appraising and Amending Theories The Strategy of .pdf}
}

@article{meehlTheoreticalRisksTabular1978,
  title = {Theoretical {{Risks}} and {{Tabular Asterisks}}: {{Sir Karl}}, {{Sir Ronald}}, and the {{Slow Progress}} of {{Soft Psychology}}},
  author = {Meehl, Paul E},
  date = {1978},
  journaltitle = {Journal of Consulting \& Clinical Psychology},
  volume = {46},
  number = {4},
  pages = {806--834},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\8YNMP9TQ\Meehl - Theoretical Risks and Tabular Asterisks.pdf}
}

@article{mischelToothbrushProblem2008,
  title = {The {{Toothbrush Problem}}},
  author = {Mischel, Walter},
  date = {2008-12-01},
  journaltitle = {APS Observer},
  volume = {21},
  url = {https://www.psychologicalscience.org/observer/the-toothbrush-problem},
  urldate = {2024-05-07},
  abstract = {In these columns, I have been discussing our “urban legends” — the often unspoken but widely shared understandings and misunderstandings about how to build a research-focused academic life in psychology. My goal is to look …},
  langid = {american},
  file = {C:\Users\vanlissa\Zotero\storage\BDLJLLY3\the-toothbrush-problem.html}
}

@article{morrisRoleFamilyContext2007,
  title = {The Role of the Family Context in the Development of Emotion Regulation},
  author = {Morris, Amanda Sheffield and Silk, Jennifer S. and Steinberg, Laurence and Myers, Sonya S. and Robinson, Lara Rachel},
  date = {2007},
  journaltitle = {Social development},
  volume = {16},
  number = {2},
  pages = {361--388},
  doi = {10.1111/j.1467-9507.2007.00389.x},
  file = {C:\Users\vanlissa\Zotero\storage\8IZJANJI\Morris et al_2007_The role of the family context in the development of emotion regulation.pdf}
}

@article{norouziCapturingCausalClaims2024,
  title = {Capturing {{Causal Claims}}: {{A Fine Tuned Text Mining Model}} for {{Extracting Causal Sentences}} from {{Social Science Papers}}},
  shorttitle = {Capturing {{Causal Claims}}},
  author = {Norouzi, Rasoul and Kleinberg, Bennett and Vermunt, Jeroen and Van Lissa, Caspar J.},
  date = {2024},
  publisher = {OSF},
  url = {https://osf.io/kwtpm/download},
  urldate = {2024-12-05},
  file = {C:\Users\vanlissa\Zotero\storage\E46IRUWL\Norouzi et al. - 2024 - Capturing Causal Claims A Fine Tuned Text Mining .pdf}
}

@article{nosekPromotingOpenResearch2015a,
  title = {Promoting an Open Research Culture},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  date = {2015-06-26},
  journaltitle = {Science},
  volume = {348},
  number = {6242},
  eprint = {26113702},
  eprinttype = {pmid},
  pages = {1422--1425},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  url = {http://science.sciencemag.org/content/348/6242/1422},
  urldate = {2019-02-07},
  abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility Author guidelines for journals could help to promote transparency, openness, and reproducibility},
  langid = {english},
  file = {C\:\\Users\\vanlissa\\Zotero\\storage\\QMIA9CDD\\Nosek et al. - 2015 - Promoting an open research culture.pdf;C\:\\Users\\vanlissa\\Zotero\\storage\\CHZTCVV2\\1422.html}
}

@article{oberauerAddressingTheoryCrisis2019,
  title = {Addressing the Theory Crisis in Psychology},
  author = {Oberauer, Klaus and Lewandowsky, Stephan},
  date = {2019-10-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {26},
  number = {5},
  pages = {1596--1618},
  issn = {1531-5320},
  doi = {10.3758/s13423-019-01645-2},
  url = {https://doi.org/10.3758/s13423-019-01645-2},
  urldate = {2022-10-20},
  abstract = {A worrying number of psychological findings are not replicable. Diagnoses of the causes of this “replication crisis,” and recommendations to address it, have nearly exclusively focused on methods of data collection, analysis, and reporting. We argue that a further cause of poor replicability is the often weak logical link between theories and their empirical tests. We propose a distinction between discovery-oriented and theory-testing research. In discovery-oriented research, theories do not strongly imply hypotheses by which they can be tested, but rather define a search space for the discovery of effects that would support them. Failures to find these effects do not question the theory. This endeavor necessarily engenders a high risk of Type I errors—that is, publication of findings that will not replicate. Theory-testing research, by contrast, relies on theories that strongly imply hypotheses, such that disconfirmation of the hypothesis provides evidence against the theory. Theory-testing research engenders a smaller risk of Type I errors. A strong link between theories and hypotheses is best achieved by formalizing theories as computational models. We critically revisit recommendations for addressing the “replication crisis,” including the proposal to distinguish exploratory from confirmatory research, and the preregistration of hypotheses and analysis plans.},
  langid = {english},
  keywords = {Computational modeling,Hypothesis testing,Preregistration,Replication,Scientific inference},
  file = {C:\Users\vanlissa\Zotero\storage\WLSKH5PR\Oberauer and Lewandowsky - 2019 - Addressing the theory crisis in psychology.pdf}
}

@article{peikertReproducibleResearchTutorial2021,
  title = {Reproducible {{Research}} in {{R}}: {{A Tutorial}} on {{How}} to {{Do}} the {{Same Thing More Than Once}}},
  shorttitle = {Reproducible {{Research}} in {{R}}},
  author = {Peikert, Aaron and Van Lissa, Caspar J. and Brandmaier, Andreas M.},
  date = {2021-12-09},
  journaltitle = {Psych},
  shortjournal = {Psych},
  volume = {3},
  number = {4},
  pages = {836--867},
  issn = {2624-8611},
  doi = {10.3390/psych3040053},
  url = {https://www.mdpi.com/2624-8611/3/4/53},
  urldate = {2022-04-28},
  abstract = {Computational reproducibility is the ability to obtain identical results from the same data with the same computer code. It is a building block for transparent and cumulative science because it enables the originator and other researchers, on other computers and later in time, to reproduce and thus understand how results came about, while avoiding a variety of errors that may lead to erroneous reporting of statistical and computational results. In this tutorial, we demonstrate how the R package repro supports researchers in creating fully computationally reproducible research projects with tools from the software engineering community. Building upon this notion of fully automated reproducibility, we present several applications including the preregistration of research plans with code (Preregistration as Code, PAC). PAC eschews all ambiguity of traditional preregistration and offers several more advantages. Making technical advancements that serve reproducibility more widely accessible for researchers holds the potential to innovate the research process and to help it become more productive, credible, and reliable.},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\9QBMR6G7\Peikert et al. - 2021 - Reproducible Research in R A Tutorial on How to D.pdf}
}

@article{quineReasonsIndeterminacyTranslation1970,
  title = {On the {{Reasons}} for {{Indeterminacy}} of {{Translation}}},
  author = {Quine, W. V.},
  date = {1970},
  journaltitle = {The Journal of Philosophy},
  volume = {67},
  number = {6},
  eprint = {2023887},
  eprinttype = {jstor},
  pages = {178--183},
  publisher = {Journal of Philosophy, Inc.},
  issn = {0022-362X},
  doi = {10.2307/2023887},
  url = {https://www.jstor.org/stable/2023887},
  urldate = {2024-11-20},
  file = {C:\Users\vanlissa\Zotero\storage\Q6HGZB86\Quine - 1970 - On the Reasons for Indeterminacy of Translation.pdf}
}

@article{ramGitCanFacilitate2013,
  title = {Git Can Facilitate Greater Reproducibility and Increased Transparency in Science},
  author = {Ram, Karthik},
  date = {2013-02-28},
  journaltitle = {Source Code for Biology and Medicine},
  shortjournal = {Source Code for Biology and Medicine},
  volume = {8},
  number = {1},
  pages = {7},
  issn = {1751-0473},
  doi = {10.1186/1751-0473-8-7},
  url = {https://doi.org/10.1186/1751-0473-8-7},
  urldate = {2020-01-08},
  abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
  file = {C\:\\Users\\vanlissa\\Zotero\\storage\\RZB63RDE\\Ram_2013_Git can facilitate greater reproducibility and increased transparency in science.pdf;C\:\\Users\\vanlissa\\Zotero\\storage\\H4UH8INM\\1751-0473-8-7.html}
}

@article{robinaughInvisibleHandsFine2021,
  title = {Invisible {{Hands}} and {{Fine Calipers}}: {{A Call}} to {{Use Formal Theory}} as a {{Toolkit}} for {{Theory Construction}}},
  shorttitle = {Invisible {{Hands}} and {{Fine Calipers}}},
  author = {Robinaugh, Donald J. and Haslbeck, Jonas M. B. and Ryan, Oisín and Fried, Eiko I. and Waldorp, Lourens J.},
  date = {2021-07-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {16},
  number = {4},
  pages = {725--743},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620974697},
  url = {https://doi.org/10.1177/1745691620974697},
  urldate = {2024-02-08},
  abstract = {In recent years, a growing chorus of researchers has argued that psychological theory is in a state of crisis: Theories are rarely developed in a way that indicates an accumulation of knowledge. Paul Meehl raised this very concern more than 40 years ago. Yet in the ensuing decades, little has improved. We aim to chart a better path forward for psychological theory by revisiting Meehl’s criticisms, his proposed solution, and the reasons his solution failed to meaningfully change the status of psychological theory. We argue that Meehl identified serious shortcomings in our evaluation of psychological theories and that his proposed solution would substantially strengthen theory testing. However, we also argue that Meehl failed to provide researchers with the tools necessary to construct the kinds of rigorous theories his approach required. To advance psychological theory, we must equip researchers with tools that allow them to better generate, evaluate, and develop their theories. We argue that formal theories provide this much-needed set of tools, equipping researchers with tools for thinking, evaluating explanation, enhancing measurement, informing theory development, and promoting the collaborative construction of psychological theories.},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\I3UI652D\Robinaugh et al. - 2021 - Invisible Hands and Fine Calipers A Call to Use F.pdf}
}

@article{scheelExcessPositiveResults2021,
  title = {An {{Excess}} of {{Positive Results}}: {{Comparing}} the {{Standard Psychology Literature With Registered Reports}}},
  shorttitle = {An {{Excess}} of {{Positive Results}}},
  author = {Scheel, Anne M. and Schijen, Mitchell R. M. J. and Lakens, Daniël},
  date = {2021-04-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {2},
  pages = {25152459211007467},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459211007467},
  url = {https://doi.org/10.1177/25152459211007467},
  urldate = {2022-04-15},
  abstract = {Selectively publishing results that support the tested hypotheses (?positive? results) distorts the available evidence for scientific claims. For the past decade, psychological scientists have been increasingly concerned about the degree of such distortion in their literature. A new publication format has been developed to prevent selective reporting: In Registered Reports (RRs), peer review and the decision to publish take place before results are known. We compared the results in published RRs (N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature (N = 152) in psychology. Analyzing the first hypothesis of each article, we found 96\% positive results in standard reports but only 44\% positive results in RRs. We discuss possible explanations for this large difference and suggest that a plausible factor is the reduction of publication bias and/or Type I error inflation in the RR literature.},
  file = {C:\Users\vanlissa\Zotero\storage\LEW9B5MX\Scheel et al_2021_An Excess of Positive Results.pdf}
}

@article{scheelWhyHypothesisTesters2021,
  title = {Why {{Hypothesis Testers Should Spend Less Time Testing Hypotheses}}},
  author = {Scheel, Anne M. and Tiokhin, Leonid and Isager, Peder M. and Lakens, Daniël},
  date = {2021-07-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {16},
  number = {4},
  pages = {744--755},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620966795},
  url = {https://doi.org/10.1177/1745691620966795},
  urldate = {2023-10-12},
  abstract = {For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance tests made research on theories in the social sciences basically uninterpretable. In response to the replication crisis, reforms in psychology have focused on formalizing procedures for testing hypotheses. These reforms were necessary and influential. However, as an unexpected consequence, psychological scientists have begun to realize that they may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established a sound “derivation chain” between test and theory is counterproductive. Instead, various nonconfirmatory research activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses, researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships between concepts and the functional form of those relationships, and identifying boundary conditions and auxiliary assumptions. Providing these inputs should be recognized and incentivized as a crucial goal in itself. In this article, we discuss how shifting the focus to nonconfirmatory research can tie together many loose ends of psychology’s reform movement and help us to develop strong, testable theories, as Paul Meehl urged.},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\EIJPIJHY\Scheel et al. - 2021 - Why Hypothesis Testers Should Spend Less Time Test.pdf}
}

@article{scheelWhyMostPsychological2022,
  title = {Why Most Psychological Research Findings Are Not Even Wrong},
  author = {Scheel, Anne M.},
  date = {2022},
  journaltitle = {Infant and Child Development},
  volume = {31},
  number = {1},
  pages = {e2295},
  issn = {1522-7219},
  doi = {10.1002/icd.2295},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/icd.2295},
  urldate = {2022-03-01},
  abstract = {Psychology's replication crisis is typically conceptualized as the insight that the published literature contains a worrying amount of unreplicable, false-positive findings. At the same time, meta-scientific attempts to assess the crisis in more detail have reported substantial difficulties in identifying unambiguous definitions of the scientific claims in published articles and determining how they are connected to the presented evidence. I argue that most claims in the literature are so critically underspecified that attempts to empirically evaluate them are doomed to failure—they are not even wrong. Meta-scientists should beware of the flawed assumption that the psychological literature is a collection of well-defined claims. To move beyond the crisis, psychologists must reconsider and rebuild the conceptual basis of their hypotheses before trying to test them.},
  langid = {english},
  keywords = {falsification,hypothesis testing,replication crisis,reproducibility,scientific inference},
  file = {C\:\\Users\\vanlissa\\Zotero\\storage\\RCSX27VL\\Scheel_2022_Why most psychological research findings are not even wrong.pdf;C\:\\Users\\vanlissa\\Zotero\\storage\\77G7YKYE\\icd.html}
}

@article{szollosiArrestedTheoryDevelopment2021,
  title = {Arrested Theory Development: {{The}} Misguided Distinction between Exploratory and Confirmatory Research},
  author = {Szollosi, Aba and Donkin, Chris},
  date = {2021},
  journaltitle = {Perspectives on Psychological Science},
  volume = {16},
  number = {4},
  pages = {717--724},
  doi = {10.1177/1745691620966796},
  abstract = {Science progresses by finding and correcting problems in theories. Good theories are those that help facilitate this process by being hard-to-vary: they explain what they are supposed to explain, they are consistent with other good theories, and they are not easily adaptable to explain anything. Here we argue that, rather than a lack of distinction between exploratory and confirmatory research, an abundance of flexible theories is a better explanation for current replicability problems of psychology. We also explain why popular methods-oriented solutions fail to address the real problem of flexibility. Instead, we propose that a greater emphasis on theory criticism by argument would improve replicability.},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\AULQYHPI\Szollosi and Donkin - Arrested theory development The misguided distinc.pdf}
}

@article{taylor2022psychology,
  title = {The Psychology of Pandemics},
  author = {Taylor, Steven},
  date = {2022},
  journaltitle = {Annual review of clinical psychology},
  volume = {18},
  number = {1},
  pages = {581--609},
  publisher = {Annual Reviews}
}

@article{vanlissaTeacherCornerEvaluating2020,
  title = {Teacher’s {{Corner}}: {{Evaluating Informative Hypotheses Using}} the {{Bayes Factor}} in {{Structural Equation Models}}},
  shorttitle = {Teacher’s {{Corner}}},
  author = {Van Lissa, Caspar J. and Gu, Xin and Mulder, Joris and Rosseel, Yves and Zundert, Camiel Van and Hoijtink, Herbert},
  date = {2020-05-29},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {0},
  number = {0},
  pages = {1--10},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2020.1745644},
  url = {https://doi.org/10.1080/10705511.2020.1745644},
  urldate = {2020-08-18},
  abstract = {This Teacher’s Corner paper introduces Bayesian evaluation of informative hypotheses for structural equation models, using the free open-source R packages bain, for Bayesian informative hypothesis testing, and lavaan, a widely used SEM package. The introduction provides a brief non-technical explanation of informative hypotheses, the statistical underpinnings of Bayesian hypothesis evaluation, and the bain algorithm. Three tutorial examples demonstrate informative hypothesis evaluation in the context of common types of structural equation models: 1) confirmatory factor analysis, 2) latent variable regression, and 3) multiple group analysis. We discuss hypothesis formulation, the interpretation of Bayes factors and posterior model probabilities, and sensitivity analysis.},
  keywords = {Bain,bayes factor,informative hypotheses,structural equation modeling}
}

@online{vanlissaUsingEndpointsCheck2023,
  type = {Package Documentation},
  title = {Using {{Endpoints}} to {{Check Reproducibility}}},
  author = {Van Lissa, Caspar J.},
  date = {2023},
  url = {https://cjvanlissa.github.io/worcs/articles/endpoints.html},
  urldate = {2024-03-21},
  abstract = {worcs},
  langid = {english},
  organization = {worcs 0.1.15.2 package documentation},
  file = {C:\Users\vanlissa\Zotero\storage\QEDWE8TZ\endpoints.html}
}

@article{vanlissaWORCSWorkflowOpen2021,
  title = {{{WORCS}}: {{A}} Workflow for Open Reproducible Code in Science},
  author = {Van Lissa, Caspar J. and Brandmaier, Andreas M. and Brinkman, Loek and Lamprecht, Anna-Lena and Peikert, Aaron and Struiksma, Marijn E. and Vreede, Barbara M.I.},
  date = {2021},
  journaltitle = {Data Science},
  volume = {4},
  number = {1},
  pages = {29--49},
  publisher = {IOS Press},
  issn = {2451-8492},
  doi = {10.3233/DS-210031},
  abstract = {Adopting open science principles can be challenging, requiring conceptual education and training in the use of new tools. This paper introduces the Workflow for Open Reproducible Code in Science (WORCS): A step-by-step procedure that researchers can follow to make a research project open and reproducible. This workflow intends to lower the threshold for adoption of open science principles. It is based on established best practices, and can be used either in parallel to, or in absence of, top-down requirements by journals, institutions, and funding bodies. To facilitate widespread adoption, the WORCS principles have been implemented in the R package worcs , which offers an RStudio project template and utility functions for specific workflow steps. This paper introduces the conceptual workflow, discusses how it meets different standards for open science, and addresses the functionality provided by the R implementation, worcs . This paper is primarily targeted towards scholars conducting research projects in R, conducting research that involves academic prose, analysis code, and tabular data. However, the workflow is flexible enough to accommodate other scenarios, and offers a starting point for customized solutions. The source code for the R package and manuscript, and a list of examplesof WORCS projects , are available at https://github.com/cjvanlissa/worcs .},
  keywords = {dynamic document generation,Open science,r,reproducibility,version control}
}

@article{vanrooijTheoryTestHow2021,
  title = {Theory {{Before}} the {{Test}}: {{How}} to {{Build High-Verisimilitude Explanatory Theories}} in {{Psychological Science}}},
  shorttitle = {Theory {{Before}} the {{Test}}},
  author = {family=Rooij, given=Iris, prefix=van, useprefix=true and Baggio, Giosuè},
  date = {2021-07-01},
  journaltitle = {Perspectives on Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {16},
  number = {4},
  pages = {682--697},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620970604},
  url = {https://doi.org/10.1177/1745691620970604},
  urldate = {2022-04-30},
  abstract = {Drawing on the philosophy of psychological explanation, we suggest that psychological science, by focusing on effects, may lose sight of its primary explananda: psychological capacities. We revisit Marr?s levels-of-analysis framework, which has been remarkably productive and useful for cognitive psychological explanation. We discuss ways in which Marr?s framework may be extended to other areas of psychology, such as social, developmental, and evolutionary psychology, bringing new benefits to these fields. We then show how theoretical analyses can endow a theory with minimal plausibility even before contact with empirical data: We call this the theoretical cycle. Finally, we explain how our proposal may contribute to addressing critical issues in psychological science, including how to leverage effects to understand capacities better.},
  file = {C:\Users\vanlissa\Zotero\storage\LW2SJ4JX\van Rooij_Baggio_2021_Theory Before the Test.pdf}
}

@online{vogtFAIR20Extending2024,
  title = {{{FAIR}} 2.0: {{Extending}} the {{FAIR Guiding Principles}} to {{Address Semantic Interoperability}}},
  shorttitle = {{{FAIR}} 2.0},
  author = {Vogt, Lars and Strömert, Philip and Matentzoglu, Nicolas and Karam, Naouel and Konrad, Marcel and Prinz, Manuel and Baum, Roman},
  date = {2024-05-06},
  eprint = {2405.03345},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2405.03345},
  urldate = {2024-11-20},
  abstract = {FAIR data presupposes their successful communication between machines and humans while preserving their meaning and reference, requiring all parties involved to share the same background knowledge. Inspired by English as a natural language, we investigate the linguistic structure that ensures reliable communication of information and draw parallels with data structures, understanding both as models of systems of interest. We conceptualize semantic interoperability as comprising terminological and propositional interoperability. The former includes ontological (i.e., same meaning) and referential (i.e., same referent/extension) interoperability and the latter schema (i.e., same data schema) and logical (i.e., same logical framework) interoperability. Since no best ontology and no best data schema exists, establishing semantic interoperability and FAIRness of data and metadata requires the provision of a comprehensive set of relevant ontological and referential entity mappings and schema crosswalks. We therefore propose appropriate additions to the FAIR Guiding Principles, leading to FAIR 2.0. Furthermore, achieving FAIRness of data requires the provision of FAIR services in addition to organizing data into FAIR Digital Objects. FAIR services include a terminology, a schema, and an operations service.},
  pubstate = {prepublished},
  keywords = {Computer Science - Databases},
  file = {C\:\\Users\\vanlissa\\Zotero\\storage\\LXGHVJ34\\Vogt et al. - 2024 - FAIR 2.0 Extending the FAIR Guiding Principles to.pdf;C\:\\Users\\vanlissa\\Zotero\\storage\\HMWZVVD9\\2405.html}
}

@unpublished{wilkinson2024applying,
      title={Applying the FAIR Principles to Computational Workflows}, 
      author={Sean R. Wilkinson and Meznah Aloqalaa and Khalid Belhajjame and Michael R. Crusoe and Bruno de Paula Kinoshita and Luiz Gadelha and Daniel Garijo and Ove Johan Ragnar Gustafsson and Nick Juty and Sehrish Kanwal and Farah Zaib Khan and Johannes Köster and Karsten Peters-von Gehlen and Line Pouchard and Randy K. Rannow and Stian Soiland-Reyes and Nicola Soranzo and Shoaib Sufi and Ziheng Sun and Baiba Vilne and Merridee A. Wouters and Denis Yuen and Carole Goble},
      year={2024},
      eprint={2410.03490},
      archivePrefix={arXiv},
      primaryClass={cs.DL},
      url={https://arxiv.org/abs/2410.03490}, 
}

@article{wilkinsonFAIRGuidingPrinciples2016a,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and family=Aalbersberg, given=IJsbrand Jan, given-i={{IJ}}J and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and family=Silva Santos, given=Luiz Bonino, prefix=da, useprefix=true and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and family=Hoen, given=Peter A. C., prefix=’t, useprefix=true and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and family=Schaik, given=Rene, prefix=van, useprefix=true and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and family=Lei, given=Johan, prefix=van der, useprefix=true and family=Mulligen, given=Erik, prefix=van, useprefix=true and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  date = {2016-03-15},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {3},
  number = {1},
  pages = {160018},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  url = {https://www.nature.com/articles/sdata201618},
  urldate = {2024-05-17},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  langid = {english},
  keywords = {Publication characteristics,Research data},
  file = {C:\Users\vanlissa\Zotero\storage\DBYTA9AD\Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf}
}
